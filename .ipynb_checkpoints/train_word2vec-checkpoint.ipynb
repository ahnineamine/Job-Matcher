{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import spacy\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import preprocess_documents\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "import io\n",
    "import unicodedata\n",
    "import multiprocessing\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has been read successfully\n"
     ]
    }
   ],
   "source": [
    "def get_data(dirname):\n",
    "    \"\"\" Collects all files in the given folder, extract each line of every file as an element, concatenate the results  \"\"\"\n",
    "    if not os.listdir(dirname):\n",
    "        print(\"Files not found-Empty Directory \")\n",
    "        return\n",
    "    else:\n",
    "        files = os.listdir(dirname) # file names i.e : cv file , job offer file\n",
    "    filenames = [dirname+\"/\"+files[i] for i in range(len(files))] # put file paths in a list\n",
    "    #train_data = [io.open(filenames[i], 'r', encoding='latin-1').read() for i in range(len(filenames))]\n",
    "    train_data=[]\n",
    "    for file in filenames:\n",
    "        with open(file,errors='ignore') as f:\n",
    "            line_list=[]\n",
    "            for l in f:\n",
    "                line = l.rstrip('\\n')\n",
    "                line_uni = unicodedata.normalize(\"NFKD\",line)\n",
    "                line_list.append(line_uni)\n",
    "        #lines = [line.rstrip('\\n') for line in open(file)]\n",
    "        train_data= train_data + line_list\n",
    "    return train_data\n",
    "\n",
    "def _keep_token(t):\n",
    "    return (t.is_alpha and \n",
    "            not (t.is_space or t.is_punct or \n",
    "                 t.is_stop or t.like_num))\n",
    "def _lemmatize_doc(doc):\n",
    "    return [ t.lemma_ for t in doc if _keep_token(t)]\n",
    "\n",
    "def spacy_preproc(document):\n",
    "    \"\"\" preprocess the documents using spacy \"\"\"\n",
    "    process_result=[]\n",
    "    for doc in document:\n",
    "         process_result.append(_lemmatize_doc(doc))\n",
    "    return process_result\n",
    "\n",
    "def gensim_preproc(document):\n",
    "    \"\"\" preprocess the documents using gensim \"\"\"\n",
    "    proc_result=[]\n",
    "    for c in document :\n",
    "        doc=gensim.utils.simple_preprocess(c)\n",
    "        proc_result.append(doc)\n",
    "    return proc_result\n",
    "\n",
    "def train_word2vec(train_data,model_name=\"word2vec_model\"):\n",
    "    \"\"\" Trains a word2vec model on the preprocessed data and saves it . \"\"\"\n",
    "    if not train_data:\n",
    "        print(\"no training data\")\n",
    "        return\n",
    "    #nlp = spacy.load('en')\n",
    "    #train_data = [nlp(doc) for doc in train_data]\n",
    "    print('data is ready for processing')\n",
    "    w2v_corpus = gensim_preproc(train_data)\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    model = Word2Vec(w2v_corpus, workers = cores-1,iter=30, size=150, sample=6e-5, alpha=0.03, min_alpha=0.0007, negative=20)\n",
    "    model.save(model_name)\n",
    "    print(\"Model Created Successfully\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_data = get_data('./data/training_data')\n",
    "    print('data has been read successfully')\n",
    "    train_word2vec(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 10:26:43: loading Word2Vec object from word2vec_model\n",
      "INFO - 10:26:45: loading wv recursively from word2vec_model.wv.* with mmap=None\n",
      "INFO - 10:26:45: setting ignored attribute vectors_norm to None\n",
      "INFO - 10:26:45: loading vocabulary recursively from word2vec_model.vocabulary.* with mmap=None\n",
      "INFO - 10:26:45: loading trainables recursively from word2vec_model.trainables.* with mmap=None\n",
      "INFO - 10:26:45: setting ignored attribute cum_table to None\n",
      "INFO - 10:26:45: loaded word2vec_model\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec.load('word2vec_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Anaconda3\\envs\\retinanet\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('knowledge', 0.6328538656234741),\n",
       " ('experienced', 0.6193768382072449),\n",
       " ('hands', 0.5870240926742554),\n",
       " ('gained', 0.5824126601219177),\n",
       " ('proficiency', 0.5733476281166077),\n",
       " ('familiarity', 0.5555065274238586),\n",
       " ('exposure', 0.5405657291412354),\n",
       " ('extensive', 0.5401183366775513),\n",
       " ('expertise', 0.5255299806594849),\n",
       " ('working', 0.5080225467681885)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('experience')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
